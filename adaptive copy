from collections import Counter
from transformers import BertTokenizer,AutoTokenizer
import numpy as np
import unicodedata
from collections import Counter
from transformers import BertTokenizer, BertModel, AutoTokenizer
import torch
from torch import nn
from torch.optim import SGD
import numpy as np
import torch
from torch import nn
from torch.optim import SGD
from tqdm import tqdm, trange


# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


def load_documents(dataset_file):
    documents = []
    words = []
    labels = []
    sentence_boundaries = []
    with open(dataset_file) as f:
        for line in f:
            line = line.rstrip()
            if line.startswith("-DOCSTART"):
                if words:
                    documents.append(dict(
                        words=words,
                        labels=labels,
                        sentence_boundaries=sentence_boundaries
                    ))
                    words = []
                    labels = []
                    sentence_boundaries = []
                continue

            if not line:
                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:
                    sentence_boundaries.append(len(words))
            else:
                items = line.split(" ")
                words.append(items[0])
                labels.append(items[-1])

    if words:
        documents.append(dict(
            words=words,
            labels=labels,
            sentence_boundaries=sentence_boundaries
        ))
        
    return documents


def load_examples(documents):
    examples = []
    max_token_length = 510
    max_mention_length = 30

    for document in tqdm(documents):
        words = document["words"]
        subword_lengths = [len(tokenizer.tokenize(w)) for w in words]
        total_subword_length = sum(subword_lengths)
        sentence_boundaries = document["sentence_boundaries"]

        for i in range(len(sentence_boundaries) - 1):
            sentence_start, sentence_end = sentence_boundaries[i:i+2]
            if total_subword_length <= max_token_length:
                # if the total sequence length of the document is shorter than the
                # maximum token length, we simply use all words to build the sequence
                context_start = 0
                context_end = len(words)
            else:
                # if the total sequence length is longer than the maximum length, we add
                # the surrounding words of the target sentenceã€€to the sequence until it
                # reaches the maximum length
                context_start = sentence_start
                context_end = sentence_end
                cur_length = sum(subword_lengths[context_start:context_end])
                while True:
                    if context_start > 0:
                        if cur_length + subword_lengths[context_start - 1] <= max_token_length:
                            cur_length += subword_lengths[context_start - 1]
                            context_start -= 1
                        else:
                            break
                    if context_end < len(words):
                        if cur_length + subword_lengths[context_end] <= max_token_length:
                            cur_length += subword_lengths[context_end]
                            context_end += 1
                        else:
                            break

            text = ""
            for word in words[context_start:sentence_start]:
                if word[0] == "'" or (len(word) == 1 and is_punctuation(word)):
                    text = text.rstrip()
                text += word
                text += " "

            sentence_words = words[sentence_start:sentence_end]
            sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]

            word_start_char_positions = []
            word_end_char_positions = []
            for word in sentence_words:
                if word[0] == "'" or (len(word) == 1 and is_punctuation(word)):
                    text = text.rstrip()
                word_start_char_positions.append(len(text))
                text += word
                word_end_char_positions.append(len(text))
                text += " "

            for word in words[sentence_end:context_end]:
                if word[0] == "'" or (len(word) == 1 and is_punctuation(word)):
                    text = text.rstrip()
                text += word
                text += " "
            text = text.rstrip()

            entity_spans = []
            original_word_spans = []
            for word_start in range(len(sentence_words)):
                for word_end in range(word_start, len(sentence_words)):
                    if sum(sentence_subword_lengths[word_start:word_end + 1]) <= max_mention_length:
                        entity_spans.append(
                            (word_start_char_positions[word_start], word_end_char_positions[word_end])
                        )
                        original_word_spans.append(
                            (word_start, word_end + 1)
                        )

            examples.append(dict(
                text=text,
                words=sentence_words,
                entity_spans=entity_spans,
                original_word_spans=original_word_spans,
            ))

    return examples



def is_punctuation(char):
    cp = ord(char)
    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):
        return True
    cat = unicodedata.category(char)
    if cat.startswith("P"):
        return True
    return False


test_documents = load_documents("eng.testb")
test_examples = load_examples(test_documents)




def compute_sequence_distribution(corpus, tokenizer, max_length=20, min_freq=1):

    seq_counter = Counter()
    
    for sentence in corpus:
        tokens = tokenizer.tokenize(sentence)
        for i in range(len(tokens)):
            for j in range(1, max_length + 1):
                if i + j <= len(tokens):
                    seq = tuple(tokens[i:i+j]) 
                    seq_counter[seq] += 1
    
  
    seq_counter = {seq: count for seq, count in seq_counter.items() if count >= min_freq}
    
    return seq_counter
all_text = [" ".join(doc["words"]) for doc in test_documents]  
base_corpus = ['The quick brown fox jumps over the lazy dog.']  
#The BERT model was pretrained on BookCorpus, a dataset consisting of 11,038

base_seq_distribution = compute_sequence_distribution(base_corpus, tokenizer)
domain_seq_distribution = compute_sequence_distribution(all_text, tokenizer)

common_sequences = [seq for seq in base_seq_distribution if seq in domain_seq_distribution]

common_tokens = [token for token in tokenizer.vocab.keys() if token in base_seq_distribution and token in domain_seq_distribution]

# print("Common tokens:", common_tokens)

total_base = sum(base_seq_distribution.values())
total_domain = sum(domain_seq_distribution.values())
base_seq_probs = {seq: count / total_base for seq, count in base_seq_distribution.items()}
domain_seq_probs = {seq: count / total_domain for seq, count in domain_seq_distribution.items()}


def calculate_kl_divergence(base_probs, domain_probs):
    kl_divergence = {}
    for seq in base_probs.keys():
        if seq in domain_probs and base_probs[seq] > 0:
            kl_divergence[seq] = domain_probs[seq] * np.log(domain_probs[seq] / base_probs[seq])
    return kl_divergence



kl_divergence = calculate_kl_divergence(base_seq_probs, domain_seq_probs)
selected_tokens = [token for token, divergence in sorted(kl_divergence.items(), key=lambda item: item[1], reverse=True)]


F_min_base = 0.001  
F_min_domain = 0.001 

N = 100  # Number of augmentations to select
L = 5  # Max length of token sequences to consider for augmentation

augmentations = []

sorted_sequences = sorted(kl_divergence.items(), key=lambda item: item[1], reverse=True)

for seq, divergence in sorted_sequences:
    seq_str = ''.join(seq)  
    if len(augmentations) >= N:
        break
    if len(seq_str) <= L and domain_seq_probs[seq] >= F_min_domain and base_seq_probs[seq] >= F_min_base:
        augmentations.append(seq_str)


embedding_size = 768  # Typical embedding size for BERT models
num_common_tokens = 100  # Number of common tokens between source and target
num_source_tokens = 50  # Number of source domain-specific tokens
num_target_tokens = 50  # Number of target domain-specific tokens
# Assuming the same embedding sizes and initializations from before
d = 768 
model = BertModel.from_pretrained('bert-base-uncased')

embedding_layer = model.get_input_embeddings()
new_tokens = augmentations
new_tokens = [tokenizer.tokenize(seq) for seq in augmentations]
new_token_ids = [tokenizer.convert_tokens_to_ids(tok_seq) for tok_seq in new_tokens]

flat_new_token_ids = list(set([item for sublist in new_token_ids for item in sublist]))

Xt = torch.randn((len(flat_new_token_ids), embedding_size))

common_token_ids = [tokenizer.convert_tokens_to_ids(seq) for seq in common_sequences if len(seq) == 1]  
common_token_ids = [item for sublist in common_token_ids for item in sublist]  
common_token_ids = list(set(common_token_ids))  

Cs = embedding_layer.weight[common_token_ids]

source_specific_tokens = [token for token in tokenizer.vocab.keys() if token not in new_tokens]
source_specific_token_ids = tokenizer.convert_tokens_to_ids(source_specific_tokens)
#### NOT SURE ABOUT THIS AS I HAD TO MAKE DIMENSIONS COMPATIBLE:
source_specific_token_ids = [i for i in source_specific_token_ids if i in common_token_ids]
Xs = embedding_layer.weight[source_specific_token_ids]


M = torch.randn((d, d), requires_grad=True)
print(M.shape, Xs.shape,torch.matmul(M, Xs.T).shape)
print(Cs.T.shape)

def loss_function(M, Cs, Xs):
    return torch.norm(torch.matmul(M, Xs.T) - Cs.T, p='fro')

optimizer = SGD([M], lr=0.01)

num_iterations = 10

for _ in range(num_iterations):
    optimizer.zero_grad() 
    loss = loss_function(M, Cs, Xs)  
    loss.backward(retain_graph=True)  
    optimizer.step()  #

Ct = torch.matmul(M, Xt.T).T
Ct = Ct.detach().numpy()

model = BertModel.from_pretrained('bert-base-uncased')

tokenizer.add_tokens(augmentations)

model.resize_token_embeddings(len(tokenizer))
new_token_embeddings = torch.tensor(Ct)

embedding_layer = model.get_input_embeddings()
num_added_tokens = len(augmentations)

embedding_layer.weight.data[-num_added_tokens:, :] = new_token_embeddings
print(new_token_embeddings)